{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc21cfd4-adcf-4024-a9b3-353a7b4be736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import math\n",
    "import wave\n",
    "import struct\n",
    "import os \n",
    "# import tarfile\n",
    "import audiolm_pytorch\n",
    "from audiolm_pytorch import AudioLMSoundStream, SoundStreamTrainer\n",
    "from audiolm_pytorch import EncodecWrapper\n",
    "from audiolm_pytorch import HubertWithKmeans, SemanticTransformer, SemanticTransformerTrainer, HubertWithKmeans, CoarseTransformer, CoarseTransformerWrapper, CoarseTransformerTrainer, FineTransformer, FineTransformerWrapper, FineTransformerTrainer, AudioLM\n",
    "from torch import nn\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.profiler import profile, record_function, ProfilerActivity, schedule\n",
    "import datetime\n",
    "import argparse\n",
    "import re\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import cocochorales_custom_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b8d79ca-e953-4385-bf09-40d59bb073c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on audiolm_pytorch version 1.2.19\n"
     ]
    }
   ],
   "source": [
    "# Usage:\n",
    "# python audiolm_pytorch_demo_laion.py --semantic=/path/to/semantic --coarse=/path/to/coarse --fine=/path/to/fine\n",
    "# Checkpoint flags are optional of course. You need to give a full path, no guarantees if it's not a full path.\n",
    "# define all dataset paths, checkpoints, etc\n",
    "prefix = \"/media/checkpoint/audiolm-pytorch-results\"\n",
    "hubert_ckpt = f'hubert/hubert_base_ls960.pt'\n",
    "hubert_quantizer = f'hubert/hubert_base_ls960_L9_km500.bin' # listed in row \"HuBERT Base (~95M params)\", column Quantizer\n",
    "\n",
    "print(f\"training on audiolm_pytorch version {audiolm_pytorch.version.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbc918e7-d0e6-4257-ac0c-d596aec65c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/checkpoint/audiolm-pytorch-results/hubert/hubert_base_ls960.pt\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "\n",
    "codec = EncodecWrapper()\n",
    "wav2vec = HubertWithKmeans(\n",
    "    # use_mert = True,\n",
    "    checkpoint_path = f\"{prefix}/{hubert_ckpt}\",\n",
    "    # checkpoint_path = None,\n",
    "    kmeans_path = f\"{prefix}/{hubert_quantizer}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ed9c5d9-deec-448b-b863-206066e013bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semantic_transformer():\n",
    "    semantic_transformer = SemanticTransformer(\n",
    "        num_semantic_tokens = wav2vec.codebook_size,\n",
    "        dim = 1024,\n",
    "        depth = 6\n",
    "    ).cuda()\n",
    "    return semantic_transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69575eb3-c635-4957-9797-27d32513c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coarse_transformer():\n",
    "    coarse_transformer = CoarseTransformer(\n",
    "        num_semantic_tokens = wav2vec.codebook_size,\n",
    "        codebook_size = 1024,\n",
    "        num_coarse_quantizers = 3,\n",
    "        dim = 512,\n",
    "        depth = 6\n",
    "    ).cuda()\n",
    "    return coarse_transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17679db0-392a-4636-a611-2f51ae23d491",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_fine_transformer():\n",
    "    fine_transformer = FineTransformer(\n",
    "        num_coarse_quantizers = 3,\n",
    "        num_fine_quantizers = 5,\n",
    "        codebook_size = 1024,\n",
    "        dim = 512,\n",
    "        depth = 6\n",
    "    ).cuda()\n",
    "    return fine_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f320363-de9e-4034-898b-86bbd6fa7bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_results_folder_path(transformer_name, prefix, results_folder_slurm_job_id):\n",
    "    assert transformer_name in {\"semantic\", \"coarse\", \"fine\"}\n",
    "    results_folder = f\"{prefix}/{transformer_name}_results_{results_folder_slurm_job_id}\"\n",
    "    return results_folder\n",
    "\n",
    "def get_potential_checkpoint_num_steps(results_folder):\n",
    "    if not os.path.exists(results_folder):\n",
    "        return None\n",
    "\n",
    "    checkpoints = [f for f in os.listdir(results_folder) if f.endswith('.pt')]\n",
    "    steps = [int(re.findall(r'\\d+', ckpt)[-1]) for ckpt in checkpoints]\n",
    "    max_step = max(steps, default=0)\n",
    "    return max_step\n",
    "\n",
    "def get_potential_checkpoint_path(transformer_name, prefix, results_folder_slurm_job_id):\n",
    "    \"\"\"Determine checkpoint paths based on the checkpoint id for the transformer specified by transformer_name and prefix. searches in `prefix` folder) or latest available checkpoints in `prefix` folder. Returns None if no such checkpoints exist at all.\"\"\"\n",
    "    results_folder = get_results_folder_path(transformer_name, prefix, results_folder_slurm_job_id)\n",
    "    max_step = get_potential_checkpoint_num_steps(results_folder)\n",
    "    return f\"{results_folder}/{transformer_name}.transformer.{max_step}.pt\" if max_step > 0 else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5e9b5dc-18e6-4529-8411-8f571bc9b13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_transformer = get_semantic_transformer()\n",
    "coarse_transformer = get_coarse_transformer()\n",
    "fine_transformer = get_fine_transformer()\n",
    "\n",
    "semantic_results_folder_suffix = str(1)\n",
    "coarse_results_folder_suffix = str(1)\n",
    "fine_results_folder_suffix = str(1)\n",
    "# sampling using one gpu only, so just load info about the transformers instead of using the trainer's load method\n",
    "semantic_ckpt = get_potential_checkpoint_path(\"semantic\", prefix, semantic_results_folder_suffix)\n",
    "coarse_ckpt = get_potential_checkpoint_path(\"coarse\", prefix, coarse_results_folder_suffix)\n",
    "fine_ckpt = get_potential_checkpoint_path(\"fine\", prefix, fine_results_folder_suffix)\n",
    "assert semantic_ckpt is not None and coarse_ckpt is not None and fine_ckpt is not None, \"all three checkpoints should exist\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48b065f7-9008-4c3a-8600-6bb712eaeb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/checkpoint/audiolm-pytorch-results/semantic_results_1/semantic.transformer.86400.pt\n",
      "/media/checkpoint/audiolm-pytorch-results/coarse_results_1/coarse.transformer.86400.pt\n",
      "/media/checkpoint/audiolm-pytorch-results/fine_results_1/fine.transformer.25200.pt\n"
     ]
    }
   ],
   "source": [
    "print(semantic_ckpt)\n",
    "print(coarse_ckpt)\n",
    "print(fine_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "404d5a6f-40ad-4289-8018-e8ba19afe225",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "semantic_transformer.load(semantic_ckpt)\n",
    "coarse_transformer.load(coarse_ckpt)\n",
    "fine_transformer.load(fine_ckpt)\n",
    "assert semantic_transformer.device == coarse_transformer.device and coarse_transformer.device == fine_transformer.device, f\"all three transformers should be on the same device. instead got semantic on {semantic_transformer.device}, coarse on {coarse_transformer.device}, and fine on {fine_transformer.device}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9afbf363-43a5-4695-92c9-e0b0769c5ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded checkpoints. sampling now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating semantic: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2048/2048 [00:33<00:00, 61.87it/s]\n",
      "generating coarse: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:12<00:00, 41.58it/s]\n",
      "generating fine: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 512/512 [02:41<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled fine token ids shape: torch.Size([1, 2560])\n",
      "num eos ids: 0\n",
      "indices of eos id: []\n",
      "AFTER MASKING OUT AFTER EOS\n",
      "num eos ids: 0\n",
      "indices of eos id: []\n",
      "mask out generated fine tokens is False\n",
      "coarse_and_fine_ids.shape torch.Size([1, 512, 8]) to be decoded from codebook indices by codec\n",
      "wav shape after codec.decode_from_codebook_indices: torch.Size([1, 1, 163840])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m fine_results_folder \u001b[38;5;241m=\u001b[39m get_results_folder_path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfine\u001b[39m\u001b[38;5;124m\"\u001b[39m, prefix, fine_results_folder_suffix)\n\u001b[1;32m     17\u001b[0m fine_num_steps \u001b[38;5;241m=\u001b[39m get_potential_checkpoint_num_steps(fine_results_folder)\n\u001b[0;32m---> 18\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/out_semantic_id_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39msemantic_checkpoint_job_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_steps_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msemantic_num_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_coarse_id_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mcoarse_checkpoint_job_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_steps_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoarse_num_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_fine_id_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mfine_checkpoint_job_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_steps_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfine_num_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m sample_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m24000\u001b[39m\n\u001b[1;32m     20\u001b[0m torchaudio\u001b[38;5;241m.\u001b[39msave(output_path, generated_wav\u001b[38;5;241m.\u001b[39mcpu(), sample_rate)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"loaded checkpoints. sampling now...\")\n",
    "# Generate output and save\n",
    "audiolm = AudioLM(\n",
    "    wav2vec = wav2vec,\n",
    "    codec = codec,\n",
    "    semantic_transformer = semantic_transformer,\n",
    "    coarse_transformer = coarse_transformer,\n",
    "    fine_transformer = fine_transformer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8602f22b-9710-4378-b577-0c3e439f54c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "generated_wav = audiolm(batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190d224c-4ab6-4ca6-bec3-174ae8907ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_wav_with_prime = audiolm(prime_wave = torch.randn(1, 320 * 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1447d7ca-15fe-4858-be72-cab372bd7b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating semantic: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2048/2048 [00:33<00:00, 61.22it/s]\n",
      "generating coarse: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:12<00:00, 41.25it/s]\n",
      "generating fine: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 512/512 [02:40<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled fine token ids shape: torch.Size([1, 2560])\n",
      "num eos ids: 0\n",
      "indices of eos id: []\n",
      "AFTER MASKING OUT AFTER EOS\n",
      "num eos ids: 0\n",
      "indices of eos id: []\n",
      "mask out generated fine tokens is False\n",
      "coarse_and_fine_ids.shape torch.Size([1, 512, 8]) to be decoded from codebook indices by codec\n",
      "wav shape after codec.decode_from_codebook_indices: torch.Size([1, 1, 163840])\n"
     ]
    }
   ],
   "source": [
    "generated_wav_with_text_condition = audiolm(text = ['chirping of birds and the distant echos of bells'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bfe03a2-d851-465d-9fe5-450b36d7f1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled. exiting.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "semantic_results_folder = get_results_folder_path(\"semantic\", prefix, semantic_results_folder_suffix)\n",
    "semantic_num_steps = get_potential_checkpoint_num_steps(semantic_results_folder)\n",
    "coarse_results_folder = get_results_folder_path(\"coarse\", prefix, coarse_results_folder_suffix)\n",
    "coarse_num_steps = get_potential_checkpoint_num_steps(coarse_results_folder)\n",
    "fine_results_folder = get_results_folder_path(\"fine\", prefix, fine_results_folder_suffix)\n",
    "fine_num_steps = get_potential_checkpoint_num_steps(fine_results_folder)\n",
    "output_path = f\"{prefix}/out_semantic_id_{semantic_results_folder_suffix}_steps_{semantic_num_steps}_coarse_id_{coarse_results_folder_suffix}_steps_{coarse_num_steps}_fine_id_{fine_results_folder_suffix}_steps_{fine_num_steps}.wav\"\n",
    "sample_rate = 24000\n",
    "torchaudio.save(output_path, generated_wav_with_text_condition.cpu(), sample_rate)\n",
    "print(\"sampled. exiting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aa4306-8c48-4485-897b-96fbb7a53ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
